%Pakete;
%A4, Report, 12pt
\documentclass[ngerman,a4paper,12pt]{scrreprt}
\usepackage[a4paper, right=20mm, left=20mm,top=30mm, bottom=30mm, marginparsep=5mm, marginparwidth=5mm, headheight=7mm, headsep=15mm,footskip=15mm]{geometry}

%Papierausrichtungen
\usepackage{pdflscape}
\usepackage{lscape}

%Deutsche Umlaute, Schriftart, Deutsche Bezeichnungen
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}

%quellcode
\usepackage{listings}

%tabellen
\usepackage{tabularx}

%listen und aufzählungen
\usepackage{paralist}

%farben
\usepackage[svgnames,table,hyperref]{xcolor}

%symbole
\usepackage{latexsym,textcomp}
\usepackage{amssymb}

%font
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}

%durch- und unterstreichen
\usepackage{ulem}

%Abkürzungsverzeichnisse
\usepackage[printonlyused]{acronym}

%Bilder
\usepackage{graphicx} %Bilder
\usepackage{float}	  %"Floating" Objects, Bilder, Tabellen...
\usepackage[space]{grffile} %Leerzechen Problem bei includegraphics
\usepackage{wallpaper} %Seitenhintergrund setzen
\usepackage{transparent} %Transparenz

%Tikz, Mindmaps, Trees
\usepackage{tikz}
\usetikzlibrary{mindmap,trees}
\usepackage{verbatim}

%for
\usepackage{forloop}
\usepackage{ifthen}

%Dokumenteigenschaften
\title{Summary AI}
\author{Tobias Blaser}
\date{\today{}, Uster}


%Kopf- /Fusszeile
\usepackage{fancyhdr}
\usepackage{lastpage}

\pagestyle{fancy}
	\fancyhf{} %alle Kopf- und Fußzeilenfelder bereinigen
	\renewcommand{\headrulewidth}{0pt} %obere Trennlinie
	\fancyfoot[L]{\jobname} %Fusszeile links
	\fancyfoot[C]{Seite \thepage/\pageref{LastPage}} %Fusszeile mitte
	\fancyfoot[R]{\today{}} %Fusszeile rechts
	\renewcommand{\footrulewidth}{0.4pt} %untere Trennlinie

%Kopf-/ Fusszeile auf chapter page
\fancypagestyle{plain} {
	\fancyhf{} %alle Kopf- und Fußzeilenfelder bereinigen
	\renewcommand{\headrulewidth}{0pt} %obere Trennlinie
	\fancyfoot[L]{\jobname} %Fusszeile links
	\fancyfoot[C]{Seite \thepage/\pageref{LastPage}} %Fusszeile mitte
	\fancyfoot[R]{\today{}} %Fusszeile rechts
	\renewcommand{\footrulewidth}{0.4pt} %untere Trennlinie
}

\usepackage{changepage}

% Abkürzungen für Kapitel, Titel und Listen
\input{commands/shortcutsListAndChapter}
\input{commands/TextStructuringBoxes}

%links, verlinktes Inhaltsverzeichnis, PDF Inhaltsverzeichnis
\usepackage[bookmarks=true,
bookmarksopen=true,
bookmarksnumbered=true,
breaklinks=true,
colorlinks=true,
linkcolor=black,
anchorcolor=black,
citecolor=black,
filecolor=black,
menucolor=black,
pagecolor=black,
urlcolor=black
]{hyperref} % Paket muss unbedingt als letzes eingebunden werden!

\usepackage{graphicx}
\begin{document}

% Inhaltsverzeichnis
\tableofcontents
\clearpage

\ch{Einführung Künstliche Intelligenz}

\expl{Periodische Dezimalzahl}{Lässt sich immer durch einen Bruch darstellen \newline
$q=\frac{z_{1} \in \mathbb{Z}}{z_{2} \in \mathbb{Z}\\\{0\}}$}

\definition{Künstliche Intelligenz}{Ist immer ein Modell der Wirklichkeit und damit eine grosse Optimierungsaufgabe. \\ $Objekt \rightarrow Modell \rightarrow Simulation (+Rauschen)$}


\definition{Antinomie}{Konflikt, der nicht lösbar ist, bsp. Barbier, der nur Leuten den Bart schneidet, die sich selbst nicht den Bart schneiden \ra Konflikt mit sich selbst}

\img{img/v2.1.jpg}{}{0.75}{}
\expl{Lernen}{Modelloptimierung durch finden optimaler Parameter.}

\ul
	\li Dame oder Schach erste Anwendungen von KI \ra finden einer optimalen Strategie.
	\li Robotik
	\li Expertensysteme (Nachbildung von Spezialwissen)
	\li Maschinelles Lernen (Konstruktion neues Wissen anhand Vorhandenem)
	\li Computerbeweise (Herleitung / Verifizierung mathematischer Formeln)
	\li Automatisches Programmieren
	\li Spracherkennung
\ulE

\expl{Aspekte der KI}{Information und Algorithmen (Verändern der geg. Informationen)}

\ch{Modellbildung}
\expl{Berechnen}{Beim Berechnen wird immer Information vernichtet \ra PC wird heiss, Energie von Bits wird ``vernichtet''.}
Modell verformt die Wirklichkeit \ra desto einfach das Modell, desto stärker die Verformung

\expl{Satz von Gödel}{Systeme, die versuchen alles zu beschreiben (axiomatisches System) führen zu einem Wiederspruch in sich selbst.}

\expl{Spiel}{Ein Spiel ist bereits ein Modell und findet in unseren Köpfen statt.}


\se{Graphische Methoden und Spieltheorie}
\img{img/v2.2.jpg}{Gerichteter Graph mit Sprague-Grundy Bewertung}{0.25}{}

\img{img/v2.3.jpg}{Rangverteilung für die Spiele Aa und Ab (links) und Rangverteilung für die Spiele
Ba und Bb (rechts)}{0.75}{}

\sse{NIM Summe}
\definition{NIM Summe}{Die NIM-Summe von zwei nat"urlichen Zahlen ist definiert als die bin"are Addition
ohne Mitnahme des Ubertragungsbits (dh. die XOR Bildung). Also: (xm, ..., xo )z(ym, .., yo ) = (zm , .., zo), wobei zi = xi + yi mod (2).}

\img{img/v2.4.jpg}{Wir bezeichnen mit z die NIM-Summe = Sprague-Grundy Rang, welcher zu zwei Zahlen (Spalten- und Zeilennummer a, b ) gehört, also azb = c. \ra Wichtig: a und b beziehen sich auf die Ränge (0...x-1) und nicht Matrixenzeilen/spaltennummer (1...x)}{0.75}{}
 Eigenschaften:
 \ul
 	\li Assoziativität: (az(bzc)) = (azb)zc
 	\li Kommutativität: azb = bza
 	\li 0 als Identität: 0zb = b
 	\li Inversit at: aza = 0
\ulE

5,7,7:
\ol
	\li Ausgangslage (Binäre Zeilennummer/Haufenanzahl): \\
		101 \\
		111 \\
		111 \\
		----- \\
		101
	\li Ich ziehe auf Gewinnerposition (Rang 0): \\
		001 \\
		111 \\
		111 \\
		-----\\
		000
\olE
Zeilen (\#Hölzchenhaufen, Dimensionen bei Gitter) werden xor verknüpft.

\sse{Matrixen}
Jeder Graph kann als Matrix dargestellt werden. \ra Nachbarschaftsmatrix \\
Spalten: von, Zeilen: nach
\ul
	\li quadratische Matrix: nichtnegative Matrix
	\li reguläre Matrix: alle Einträge A\^k sin dpositiv
\ulE

%Skizze s2.1


\ch{Repetition}
Fehler kann nie null sein, aber kann möglichst klein sein.

\expl{Strategie}{Definiert, wie minimaler Fehler erreicht werden kann.}

\expl{Sprague-Grundi-Ränge (Spieltheorie)}{Felder werden mit möglichst kleiner Zahl ausgefüllt, die nicht schon in einem direkten Nachfolgefeld steht. \\ \ra wenn ich auf einem Rang-0 Feld bin, kann der Andere nur auf eine Nicht-0 Zahl ziehen. Danach kann ich entweder auf die Endnull oder ein anderes 0-Feld ziehen. \\ \textbf{\ra Zyklen dürfen nicht vorkommen}}

\expl{NIM-Spiel}{Durch XOR Verknüpfung der Binären Haufenanzahl werden 0-Positionen ermittelt}

\expl{Matrixendarstellung von Graphen}{\ol 
	\li Durchnummerieren der Knoten 
	\li erstellen einer Adjazenzmatrix (Nachbarschaftsmatrix) die n*n Felder besitzt (n=\#Knoten) 
	\li Die Kanten werden als von-zu mit 1 in der Matrix eingetragen 
\olE

Durch multiplizieren eines Punktes mit sich selbst \ra Wenn es eine Einheitsmatrix gibt, ist der Graph nicht zusammenhängend
}
Punkt mit sich selbst multiplizieren (Mathematica):
\begin{verbatim}
A = {{1,0}{1,1}};
A.A
{{1,0}{2,1}};
A.A.A.A.A
\end{verbatim}

\expl{Reguläre Matrix}{Von jedem Punkt aus ist jeder Punkt erreichbar.}


\ch{Lineare Optimierungen}
Nebenbedingungen und Gewinne sind lineare Funktionen.\\
\exam{Simplexverfahren ist nicht prüfungsrelevant}


\se{Quadratische Optimierungen}


\se{Lagrange Multiplikatoren}


\ch{Fraktale Komprimierung}
\se{Sierpinslei-Gitter}


\se{Repetition}
\ul
	\li Warum ist AI so kompliziert?
		\ul
			\li Weil es viele Methoden gibt
			\li Ein gutes Modell ist bedingt für eine entsprehende Optimierungslösung
			\li Es müssen optimale Parameter gefunden werden. \img{img/v5.1.jpg}{}{0.5}{}
			\ra wenn das Diagramm einen einfach Kurve wäre, so liesse sich die Lösung mittels Gradientenabstieg finden. Weiles ncht so ist, muss die Lösung durch ``Springen'' von Tiefpunkt zu Tiefpunkt.
		\ulE
	\li Fraktale: Konkurrenzkampf zwischen drei Punkten führt zu immer kleinereren Strukturen.
\ulE



\ch{Neuronale Netze}
\img{img/v6.1.jpg}{Neuron}{0.5}{}
\img{img/v6.2.jpg}{Hodgkin-Huxley Schema}{0.25}{}
\img{img/v6.3.jpg}{Neuroimpulse nach dem Hudgkin-Huxley Modell}{0.5}{}
\img{img/v6.4.jpg}{Ausgabefunktion Soma\ra $f(\sum{x_i*w_i})$ für AND. Damit ein Spike ausgelöst wird, muss die Multiplikation der beiden Eingänge ungleich 0 sein \ra somit wird nur gefeuert, wenn beide Eingänge 1 sind.}{0.5}{}
\expl{Fehlerbild}{Darstellung der Abweichung zu 1 vieler Versuche eines AND-Neurons}

\exam{Biologische Neurovertiefung kommt nicht an Prüfung}
\definition{Lernen}{Eine Synapse, wenn ihre Aktivität zu einem Feuern seines Zielneurons führt, wird
verstäarkt, andernfalls wird sie geschwächt}

\se{Perzeptron}
\definition{Prezeptron}{Mathematisches Neuron}
\expl{Gleichung}{Perzeptron wird durch Geradengleichung dargestellt. Daher sind alle 16 Op's ausser XOR und NXOR möglich. (N)XOR benötigt zwei Trennungen auf dem Zahlenstrahl, was mit einer linearen Gleichung nicht darstellbar ist.}

\noindent Die Mathematische Sprungfunktion wird verwendet zur Differenzierung der Werte. \\
\begin{figure}
\[ -1/(1 + e^(a*x))+0.5 \]
\caption{Sprungfunktion}
\end{figure}
Desto grösser a gewählt wird, desto aprupter wechselt die Funktion die Richtung.

\img{img/v6.6.jpg}{Perzeptron \{A,B,C\} \ra \{0,1,1\}:0, \{0,0,1\}: 0, \{1,0,1\}:0, \{1,1,1\}: 1 }{0.5}{}

\sse{Werbos Perzeptron}
\definition{Werbos Perzeptron}{Ist eigentlich ein Neuronales Netz weil es aus vier Neuronen besteht}

\img{img/v6.7.jpg}{Werbos Perzeptron \{A,B,C,D\}, D=A*B \ra \{0,1,1,0\}:0, \{0,0,1,0\}: 0, \{1,0,1,0\}:0, \{1,1,1,1\}: 1 }{0.5}{}
\img{img/v6.5.jpg}{}{0.5}{}

Die Gleichung eines Werbos Perzeptrons ist gleich der Gleichung eines Kegelschnittes / Parabel: $3x_1+2x_2+7x_1*x_x2-3=0$


\ch{Neuronale Netze}
\img{img/v7.1.jpg}{Neuronale Netze mit Zwischenschichten}{0.5}{}
\sse{Kortex}
\img{img/v7.2.jpg}{Kortex Schichten: 6 Schichten(5 sichtbar)}{0.5}{}
Jeder Layer ist ein in sich funktionierendes Neuronales Netz. Der Körper braucht zwei Schichten, um Eingaben zu verarbeiten und Ausgaben zu steuern.

\img{img/v7.3.jpg}{konvexe Akzeptanzgebiete}{0.5}{}
\sse{Schichten}
\uli{
	\li 1 Schicht erlaubt konvexe Akzeptanzgebiete. Mehrere Getrennte Gebiete sind nicht möglich.
	\li 2 Schichten erlauben das Kombinieren von mehreren Formen
}
\img{img/v7.4.jpg}{konkave Akzeptanzgebiete}{0.5}{}

\se{Lernen - Wie werden die Gewichte richtig eingestellt?}
\definition{Back Propagation Network}{Fehlerfortpflanzungsnetzwerk}


\ch{Holographisches Perzeptron (6.8)}


\se{Repetition}	
\exam{Wissen, das mit Langrange Multiplikatoren Nebenbedingungen eingebaut werden}
\exam{Support Vektormaschine: Nur wissen, was es bringt \ra kein besseres Perzeptron, dafür stabilität weil sich Grenze genau in Mitte zwischen Punkten befindet.}


\ch{Selbstorganisierende Kohonennetzwerke}
\img{img/v9.1.jpg}{Finden eines globalen Maximums}{0.75}{}
\uli{
	\li Ziel ist es, von einem lokalen Minimum zu einem globalen Minimum zu finden
	\li Mischung zwischen Gradientenabstieg und Zufallssuche
	\li Vorgehen: 
		\oli{
			\li Neuronen liegen als Netz am Kneuel (zufällig)
			\li Zielpunkt zufällig wählen
			\li Nächstes Neuron zum Zielpunkt ziehen
			\li Nachbarelemente an imaginärem Gummifaden mitziehen
			\li 2-4 wiederholen
		}
	\li Auch wenn ein Neuron an seinem Zielpunkt angelangt ist, kann es wieder davon weggezogen werden, sobald ein Nachbarneuron zu seinem Platz strebt \ra Zielpunkte werden nie genau erreicht, aber angenähert.
	\li Das Ergebnis ist nicht unbedingt die absolut beste Lösung, aber eine Gute.
	\li Wendet man das gleicher Verfahren mehrmals an mit verschiedenen Knäuel so kann man das kleinste auswählen. 
	\li Erhält man mehrmals die Gleich, so kann man davon ausgehen dass es eine sehr gute ist
	\li Schwierigkeit besteht darin, das reale Problem auf ein Kohonennetz abzubilden
}


\ch{Monte Carlo Funktion}
\uli{
	\li Gradientenabspieg mit  ``Schütteln''
	\li Berechnet man ein besseres Optimum, so wird dieses als neues Optimum gewählt
	\li Damit man aus einem lokaln Minimum rausspringen kann, wählt man mit einer kleinen Wahrscheinlichkeit das neue Optimum, auch wenn es schlechter als das Vorherige ist
	\li Desto schlechter das neue Optimum, desto unahrscheinlicher seine Wahl.
}

\expl{Rüttelverfahren}{
	\uli{
		\li Für alte Lösung wird Qualität berechnet
		\li Mit der alten Lösung wird mit möglichst kleiner Änderung eine neue Lösung berechnet \ra\ ist sie Besser, wird sie meistens genommen.
		\li Mit einer Wahrscheinlichkeit, die vom Abstand der Güte abhängt, wird trotzdem die schlechtere gewählt, was das Springen über ein lokales Maximum ermöglicht
	}
}
\img{img/v9.2.jpg}{Gradientenabstieg mit Rüttelverfahren}{0.5}{}

\expl{Probabilistische Optimierungsmethoden}{
Zwei Komponenten:
	\uli{
		\li Schrittweise verbesserug: Gradientenabstieg \ra\ Behalten, was gut war
		\li Schütteln \ra\ Struktur aufbrechen
	}
}


\ch{Hopfield Network}
\definition{rekurent}{ungerichtetes Netzwerk}
\expl{Hopfield Netzwerk}{Das Hopfield Netzwerk lernt nicht, sondern setzt die Gewichte anhand des eingegebenen Musters.}



\ch{Repetition}
\img{img/v5.1.jpg}{}{0.5}{}
\se{Lineare Trennverfahren}
\sse{Eigenschaften}
\uli{
	\li Gradientenabstieg
	\li Überwacht
	\li vorwärtsgerichtete Netze
}
\sse{Verfahren}
\uli{
	\li klassisches Perzeptron \ra\ für klassisches Gradientenabstiegs-Lernen
	\li holographisches Neuron \ra\ neuformulierungen des klassischen Perzeptron-Prinzipes
}

\se{Nicht-Lineare Trennverfahren}
\sse{Eigenschaften}
\uli{
	\li Versteckter Gradientenabstieg
	\li Rütteln (Temperatureinfluss)
	\li Selbstorganisiert
	\li rekurentes Netzwerk
}

\sse{Verfahren}
\uli{
	\li Monte-Carlo erfahren
	\li Kohonen-Netze
	\li Hopfield Netz
}



\ch{Effizienzbetrachtungen}
\uli{
	\li Erkennung von Translationen und Rotationen von Bilder sind möglich
	\li Skalierung ist nur theorethisch möglich, in der Praxisch scheitert das Problem an der Rechengenauigkeit der Numerik
}



\ch{Genetische Algorithmen}
\expl{Monte Carlo}{Gute Lösungen werden nur leicht abgeändert, neue Varianten werden ausprobiert}
\stdImg{v11.1}{Fitnessfunktion}
\uli{
	\li Genetische Algorithmen brauchen immer eine Fitnessfunktion, die optimiert werden kann
	\li Fitnessfunktionen haben oft viele locale minimas \ra\ Gradientenabstieg kommt nicht zum Ziel
}








\end{document}
