%Pakete;
%A4, Report, 12pt
\documentclass[ngerman,a4paper,12pt]{scrreprt}
\usepackage[a4paper, right=20mm, left=20mm,top=30mm, bottom=30mm, marginparsep=5mm, marginparwidth=5mm, headheight=7mm, headsep=15mm,footskip=15mm]{geometry}

%Papierausrichtungen
\usepackage{pdflscape}
\usepackage{lscape}

%Deutsche Umlaute, Schriftart, Deutsche Bezeichnungen
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}

%quellcode
\usepackage{listings}

%tabellen
\usepackage{tabularx}

%listen und aufzählungen
\usepackage{paralist}

%farben
\usepackage[svgnames,table,hyperref]{xcolor}

%symbole
\usepackage{latexsym,textcomp}
\usepackage{amssymb}

%font
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}

%durch- und unterstreichen
\usepackage{ulem}

%Abkürzungsverzeichnisse
\usepackage[printonlyused]{acronym}

%Bilder
\usepackage{graphicx} %Bilder
\usepackage{float}	  %"Floating" Objects, Bilder, Tabellen...
\usepackage[space]{grffile} %Leerzechen Problem bei includegraphics
\usepackage{wallpaper} %Seitenhintergrund setzen
\usepackage{transparent} %Transparenz

%Tikz, Mindmaps, Trees
\usepackage{tikz}
\usetikzlibrary{mindmap,trees}
\usepackage{verbatim}

%for
\usepackage{forloop}
\usepackage{ifthen}

%Dokumenteigenschaften
\title{Summary AI}
\author{Tobias Blaser}
\date{\today{}, Uster}


%Kopf- /Fusszeile
\usepackage{fancyhdr}
\usepackage{lastpage}

\pagestyle{fancy}
	\fancyhf{} %alle Kopf- und Fußzeilenfelder bereinigen
	\renewcommand{\headrulewidth}{0pt} %obere Trennlinie
	\fancyfoot[L]{\jobname} %Fusszeile links
	\fancyfoot[C]{Seite \thepage/\pageref{LastPage}} %Fusszeile mitte
	\fancyfoot[R]{\today{}} %Fusszeile rechts
	\renewcommand{\footrulewidth}{0.4pt} %untere Trennlinie

%Kopf-/ Fusszeile auf chapter page
\fancypagestyle{plain} {
	\fancyhf{} %alle Kopf- und Fußzeilenfelder bereinigen
	\renewcommand{\headrulewidth}{0pt} %obere Trennlinie
	\fancyfoot[L]{\jobname} %Fusszeile links
	\fancyfoot[C]{Seite \thepage/\pageref{LastPage}} %Fusszeile mitte
	\fancyfoot[R]{\today{}} %Fusszeile rechts
	\renewcommand{\footrulewidth}{0.4pt} %untere Trennlinie
}

\usepackage{changepage}

% Abkürzungen für Kapitel, Titel und Listen
\input{commands/shortcutsListAndChapter}
\input{commands/TextStructuringBoxes}

%links, verlinktes Inhaltsverzeichnis, PDF Inhaltsverzeichnis
\usepackage[bookmarks=true,
bookmarksopen=true,
bookmarksnumbered=true,
breaklinks=true,
colorlinks=true,
linkcolor=black,
anchorcolor=black,
citecolor=black,
filecolor=black,
menucolor=black,
pagecolor=black,
urlcolor=black
]{hyperref} % Paket muss unbedingt als letzes eingebunden werden!

\usepackage{graphicx}
\begin{document}

% Inhaltsverzeichnis
\tableofcontents
\clearpage

\ch{Einführung Künstliche Intelligenz}

\expl{Periodische Dezimalzahl}{Lässt sich immer durch einen Bruch darstellen \newline
$q=\frac{z_{1} \in \mathbb{Z}}{z_{2} \in \mathbb{Z}\\\{0\}}$}

\definition{Künstliche Intelligenz}{Ist immer ein Modell der Wirklichkeit und damit eine grosse Optimierungsaufgabe. \\ $Objekt \rightarrow Modell \rightarrow Simulation (+Rauschen)$}


\definition{Antinomie}{Konflikt, der nicht lösbar ist, bsp. Barbier, der nur Leuten den Bart schneidet, die sich selbst nicht den Bart schneiden \ra Konflikt mit sich selbst}

\img{img/v2.1.jpg}{}{0.75}{}
\expl{Lernen}{Modelloptimierung durch finden optimaler Parameter.}

\ul
	\li Dame oder Schach erste Anwendungen von KI \ra finden einer optimalen Strategie.
	\li Robotik
	\li Expertensysteme (Nachbildung von Spezialwissen)
	\li Maschinelles Lernen (Konstruktion neues Wissen anhand Vorhandenem)
	\li Computerbeweise (Herleitung / Verifizierung mathematischer Formeln)
	\li Automatisches Programmieren
	\li Spracherkennung
\ulE

\expl{Aspekte der KI}{Information und Algorithmen (Verändern der geg. Informationen)}

\ch{Modellbildung}
\expl{Berechnen}{Beim Berechnen wird immer Information vernichtet \ra PC wird heiss, Energie von Bits wird ``vernichtet''.}
Modell verformt die Wirklichkeit \ra desto einfach das Modell, desto stärker die Verformung

\expl{Satz von Gödel}{Systeme, die versuchen alles zu beschreiben (axiomatisches System) führen zu einem Wiederspruch in sich selbst.}

\expl{Spiel}{Ein Spiel ist bereits ein Modell und findet in unseren Köpfen statt.}


\se{Graphische Methoden und Spieltheorie}
\img{img/v2.2.jpg}{Gerichteter Graph mit Sprague-Grundy Bewertung}{0.25}{}

\img{img/v2.3.jpg}{Rangverteilung für die Spiele Aa und Ab (links) und Rangverteilung für die Spiele
Ba und Bb (rechts)}{0.75}{}

\sse{NIM Summe}
\definition{NIM Summe}{Die NIM-Summe von zwei nat"urlichen Zahlen ist definiert als die bin"are Addition
ohne Mitnahme des Ubertragungsbits (dh. die XOR Bildung). Also: (xm, ..., xo )z(ym, .., yo ) = (zm , .., zo), wobei zi = xi + yi mod (2).}

\img{img/v2.4.jpg}{Wir bezeichnen mit z die NIM-Summe = Sprague-Grundy Rang, welcher zu zwei Zahlen (Spalten- und Zeilennummer a, b ) gehört, also azb = c. \ra Wichtig: a und b beziehen sich auf die Ränge (0...x-1) und nicht Matrixenzeilen/spaltennummer (1...x)}{0.75}{}
 Eigenschaften:
 \ul
 	\li Assoziativität: (az(bzc)) = (azb)zc
 	\li Kommutativität: azb = bza
 	\li 0 als Identität: 0zb = b
 	\li Inversit at: aza = 0
\ulE

5,7,7:
\ol
	\li Ausgangslage (Binäre Zeilennummer/Haufenanzahl): \\
		101 \\
		111 \\
		111 \\
		----- \\
		101
	\li Ich ziehe auf Gewinnerposition (Rang 0): \\
		001 \\
		111 \\
		111 \\
		-----\\
		000
\olE
Zeilen (\#Hölzchenhaufen, Dimensionen bei Gitter) werden xor verknüpft.

\sse{Matrixen}
Jeder Graph kann als Matrix dargestellt werden. \ra Nachbarschaftsmatrix \\
Spalten: von, Zeilen: nach
\ul
	\li quadratische Matrix: nichtnegative Matrix
	\li reguläre Matrix: alle Einträge A\^k sin dpositiv
\ulE

%Skizze s2.1


\ch{Repetition}
Fehler kann nie null sein, aber kann möglichst klein sein.

\expl{Strategie}{Definiert, wie minimaler Fehler erreicht werden kann.}

\expl{Sprague-Grundi-Ränge (Spieltheorie)}{Felder werden mit möglichst kleiner Zahl ausgefüllt, die nicht schon in einem direkten Nachfolgefeld steht. \\ \ra wenn ich auf einem Rang-0 Feld bin, kann der Andere nur auf eine Nicht-0 Zahl ziehen. Danach kann ich entweder auf die Endnull oder ein anderes 0-Feld ziehen. \\ \textbf{\ra Zyklen dürfen nicht vorkommen}}

\expl{NIM-Spiel}{Durch XOR Verknüpfung der Binären Haufenanzahl werden 0-Positionen ermittelt}

\expl{Matrixendarstellung von Graphen}{\ol 
	\li Durchnummerieren der Knoten 
	\li erstellen einer Adjazenzmatrix (Nachbarschaftsmatrix) die n*n Felder besitzt (n=\#Knoten) 
	\li Die Kanten werden als von-zu mit 1 in der Matrix eingetragen 
\olE

Durch multiplizieren eines Punktes mit sich selbst \ra Wenn es eine Einheitsmatrix gibt, ist der Graph nicht zusammenhängend
}
Punkt mit sich selbst multiplizieren (Mathematica):
\begin{verbatim}
A = {{1,0}{1,1}};
A.A
{{1,0}{2,1}};
A.A.A.A.A
\end{verbatim}

\expl{Reguläre Matrix}{Von jedem Punkt aus ist jeder Punkt erreichbar.}


\ch{Lineare Optimierungen}
Nebenbedingungen und Gewinne sind lineare Funktionen.\\
\exam{Simplexverfahren ist nicht prüfungsrelevant}


\se{Quadratische Optimierungen}


\se{Lagrange Multiplikatoren}


\ch{Fraktale Komprimierung}
\se{Sierpinslei-Gitter}


\se{Repetition}
\ul
	\li Warum ist AI so kompliziert?
		\ul
			\li Weil es viele Methoden gibt
			\li Ein gutes Modell ist bedingt für eine entsprehende Optimierungslösung
			\li Es müssen optimale Parameter gefunden werden. \img{img/v5.1.jpg}{}{0.5}{}
			\ra wenn das Diagramm einen einfach Kurve wäre, so liesse sich die Lösung mittels Gradientenabstieg finden. Weiles ncht so ist, muss die Lösung durch ``Springen'' von Tiefpunkt zu Tiefpunkt.
		\ulE
	\li Fraktale: Konkurrenzkampf zwischen drei Punkten führt zu immer kleinereren Strukturen.
\ulE



\ch{Neuronale Netze}
\img{img/v6.1.jpg}{Neuron}{0.5}{}
\img{img/v6.2.jpg}{Hodgkin-Huxley Schema}{0.25}{}
\img{img/v6.3.jpg}{Neuroimpulse nach dem Hudgkin-Huxley Modell}{0.5}{}
\img{img/v6.4.jpg}{Ausgabefunktion Soma\ra $f(\sum{x_i*w_i})$ für AND. Damit ein Spike ausgelöst wird, muss die Multiplikation der beiden Eingänge ungleich 0 sein \ra somit wird nur gefeuert, wenn beide Eingänge 1 sind.}{0.5}{}
\expl{Fehlerbild}{Darstellung der Abweichung zu 1 vieler Versuche eines AND-Neurons}

\exam{Biologische Neurovertiefung kommt nicht an Prüfung}
\definition{Lernen}{Eine Synapse, wenn ihre Aktivität zu einem Feuern seines Zielneurons führt, wird
verstäarkt, andernfalls wird sie geschwächt}

\se{Perzeptron}
\definition{Prezeptron}{Mathematisches Neuron}
\expl{Gleichung}{Perzeptron wird durch Geradengleichung dargestellt. Daher sind alle 16 Op's ausser XOR und NXOR möglich. (N)XOR benötigt zwei Trennungen auf dem Zahlenstrahl, was mit einer linearen Gleichung nicht darstellbar ist.}

\noindent Die Mathematische Sprungfunktion wird verwendet zur Differenzierung der Werte. \\
\begin{figure}
\[ -1/(1 + e^(a*x))+0.5 \]
\caption{Sprungfunktion}
\end{figure}
Desto grösser a gewählt wird, desto aprupter wechselt die Funktion die Richtung.

\img{img/v6.6.jpg}{Perzeptron \{A,B,C\} \ra \{0,1,1\}:0, \{0,0,1\}: 0, \{1,0,1\}:0, \{1,1,1\}: 1 }{0.5}{}

\sse{Werbos Perzeptron}
\definition{Werbos Perzeptron}{Ist eigentlich ein Neuronales Netz weil es aus vier Neuronen besteht}

\img{img/v6.7.jpg}{Werbos Perzeptron \{A,B,C,D\}, D=A*B \ra \{0,1,1,0\}:0, \{0,0,1,0\}: 0, \{1,0,1,0\}:0, \{1,1,1,1\}: 1 }{0.5}{}
\img{img/v6.5.jpg}{}{0.5}{}

Die Gleichung eines Werbos Perzeptrons ist gleich der Gleichung eines Kegelschnittes / Parabel: $3x_1+2x_2+7x_1*x_x2-3=0$


\ch{Neuronale Netze}
\img{img/v7.1.jpg}{Neuronale Netze mit Zwischenschichten}{0.5}{}
\sse{Kortex}
\img{img/v7.2.jpg}{Kortex Schichten: 6 Schichten(5 sichtbar)}{0.5}{}
Jeder Layer ist ein in sich funktionierendes Neuronales Netz. Der Körper braucht zwei Schichten, um Eingaben zu verarbeiten und Ausgaben zu steuern.

\img{img/v7.3.jpg}{konvexe Akzeptanzgebiete}{0.5}{}
\sse{Schichten}
\uli{
	\li 1 Schicht erlaubt konvexe Akzeptanzgebiete. Mehrere Getrennte Gebiete sind nicht möglich.
	\li 2 Schichten erlauben das Kombinieren von mehreren Formen
}
\img{img/v7.4.jpg}{konkave Akzeptanzgebiete}{0.5}{}

\se{Lernen - Wie werden die Gewichte richtig eingestellt?}
\definition{Back Propagation Network}{Fehlerfortpflanzungsnetzwerk}


\ch{Holographisches Perzeptron (6.8)}


\se{Repetition}	
\exam{Wissen, das mit Langrange Multiplikatoren Nebenbedingungen eingebaut werden}
\exam{Support Vektormaschine: Nur wissen, was es bringt \ra kein besseres Perzeptron, dafür stabilität weil sich Grenze genau in Mitte zwischen Punkten befindet.}


\ch{Selbstorganisierende Kohonennetzwerke}
\img{img/v9.1.jpg}{Finden eines globalen Maximums}{0.75}{}
\uli{
	\li Ziel ist es, von einem lokalen Minimum zu einem globalen Minimum zu finden
	\li Mischung zwischen Gradientenabstieg und Zufallssuche
	\li Vorgehen: 
		\oli{
			\li Neuronen liegen als Netz am Kneuel (zufällig)
			\li Zielpunkt zufällig wählen
			\li Nächstes Neuron zum Zielpunkt ziehen
			\li Nachbarelemente an imaginärem Gummifaden mitziehen
			\li 2-4 wiederholen
		}
	\li Auch wenn ein Neuron an seinem Zielpunkt angelangt ist, kann es wieder davon weggezogen werden, sobald ein Nachbarneuron zu seinem Platz strebt \ra Zielpunkte werden nie genau erreicht, aber angenähert.
	\li Das Ergebnis ist nicht unbedingt die absolut beste Lösung, aber eine Gute.
	\li Wendet man das gleicher Verfahren mehrmals an mit verschiedenen Knäuel so kann man das kleinste auswählen. 
	\li Erhält man mehrmals die Gleich, so kann man davon ausgehen dass es eine sehr gute ist
	\li Schwierigkeit besteht darin, das reale Problem auf ein Kohonennetz abzubilden
}


\ch{Monte Carlo Funktion}
\uli{
	\li Gradientenabspieg mit  ``Schütteln''
	\li Berechnet man ein besseres Optimum, so wird dieses als neues Optimum gewählt
	\li Damit man aus einem lokaln Minimum rausspringen kann, wählt man mit einer kleinen Wahrscheinlichkeit das neue Optimum, auch wenn es schlechter als das Vorherige ist
	\li Desto schlechter das neue Optimum, desto unahrscheinlicher seine Wahl.
}

\expl{Rüttelverfahren}{
	\uli{
		\li Für alte Lösung wird Qualität berechnet
		\li Mit der alten Lösung wird mit möglichst kleiner Änderung eine neue Lösung berechnet \ra\ ist sie Besser, wird sie meistens genommen.
		\li Mit einer Wahrscheinlichkeit, die vom Abstand der Güte abhängt, wird trotzdem die schlechtere gewählt, was das Springen über ein lokales Maximum ermöglicht
	}
}
\img{img/v9.2.jpg}{Gradientenabstieg mit Rüttelverfahren}{0.5}{}

\expl{Probabilistische Optimierungsmethoden}{
Zwei Komponenten:
	\uli{
		\li Schrittweise verbesserug: Gradientenabstieg \ra\ Behalten, was gut war
		\li Schütteln \ra\ Struktur aufbrechen
	}
}


\ch{Hopfield Network}
\definition{rekurent}{ungerichtetes Netzwerk}
\expl{Hopfield Netzwerk}{Das Hopfield Netzwerk lernt nicht, sondern setzt die Gewichte anhand des eingegebenen Musters.}



\ch{Repetition}
\img{img/v5.1.jpg}{}{0.5}{}
\se{Lineare Trennverfahren}
\sse{Eigenschaften}
\uli{
	\li Gradientenabstieg
	\li Überwacht
	\li vorwärtsgerichtete Netze
}
\sse{Verfahren}
\uli{
	\li klassisches Perzeptron \ra\ für klassisches Gradientenabstiegs-Lernen
	\li holographisches Neuron \ra\ neuformulierungen des klassischen Perzeptron-Prinzipes
}

\se{Nicht-Lineare Trennverfahren}
\sse{Eigenschaften}
\uli{
	\li Versteckter Gradientenabstieg
	\li Rütteln (Temperatureinfluss)
	\li Selbstorganisiert
	\li rekurentes Netzwerk
}

\sse{Verfahren}
\uli{
	\li Monte-Carlo erfahren
	\li Kohonen-Netze
	\li Hopfield Netz
}



\ch{Effizienzbetrachtungen}
\uli{
	\li Erkennung von Translationen und Rotationen von Bilder sind möglich
	\li Skalierung ist nur theorethisch möglich, in der Praxisch scheitert das Problem an der Rechengenauigkeit der Numerik
}



\ch{Genetische Algorithmen}
\expl{Monte Carlo}{Gute Lösungen werden nur leicht abgeändert, neue Varianten werden ausprobiert}
\stdImg{v11.1}{Fitnessfunktion}
\uli{
	\li Genetische Algorithmen brauchen immer eine Fitnessfunktion, die optimiert werden kann
	\li Fitnessfunktionen haben oft viele locale minimas \ra\ Gradientenabstieg kommt nicht zum Ziel
}

\se{Repetition}
\expl{Monte Carlo}{Man wählt eine zufällige Lösung, wenn die neue Lösung besser ist als die Alte, wird sie genommen, wenn sie schlechter ist, wird sie mit einer kleinen Wahrscheinlichkeit (abhängig von der Qualität der neuen Lösung) trotzdem genommen.\\
\ra\ Neue Lösung wird gebldet aus alter Lösung: Gradientenabstieg
\ra\ Teilweises wegwerfen: Springen über lokale Maxima}

\expl{Genetische Algorithmen}{
\uli{
	\li Elternauswahl durch Glücksrad: bilden einer neuen Lösung aus der Alten (Gute Eltern haben grössere Wahrscheinlichkeit)
	\li Kreuzung: übernehmen guter Elemente aus alter Lösung
	\li Mutation: Bilden von neuer Information \ra\ Überspringen lokaler Maxima
}
}



\se{Genetische Programmierung}
\definition{Genetische Programmierung}{Genetische Erzeugung eines optimalen Programms}
\expl{Genetische Programmierung}{Operationen werden zufälig gewählt und deren Kombination mit einem genetischen Algorithmus verbessert.}
\begin{verbatim}
randomp := Block[{r = Random[Integer, {1, 26}]}, Which[
  r <= 3, If[randomp, Evaluate[randomp], Evaluate[randomp]],
  r <= 6, And[randomp, randomp],
  r <= 9, Or[randomp, randomp],
  r <= 12, Not[randomp],
  r == 13, e,
  r == 14, se,
  r == 15, s,
  r == 16,sw,
  r == 17, w,
  r == 18, nw,
  r == 19, n,
  r == 20, ne,
  r == 21, east,
  r == 22, south,
  r == 23, west,
  r == 24, north,
  r == 25, True,
  r == 26, False]]
Do[program[i]=randomp,{i,1000}]
\end{verbatim}


\se{Clustering Gruppierverfahren}
\expl{Clustering}{Aufteilen von Parametern in Gruppen. Die besten Gruppen anschliessend wiederum aufteilen.}
Um Konkave und Konvexe Trennungen zu ermöglichen, müssen die Punkte Punkt-zu-Punkt berechnet werden, und nicht Punkt-Gruppe.

\se{Exam}
\uli{
	\li Mathematica Programme lesen können
	\li Do Loop verstehen
	\li Fehlerlandschaft erklären Können (mit/ohne Gradientenabstieg lösbar)
	\li Satz von Göden (Fehler kann nicht auf 0 gebracht werden können). Aus dem System heraus lässt sich nicht das ganze System beschreiben. Man kann immer nur eine Abstraktion der Realität machen.
	\li Spieltheorie
		\uli{
			\li Dodon's Problem
			\li Treibt die Dame in die Ecke, Graphen zeichnen und ausfüllen können (Sprague/Grundi) [2P]
			\li Hölzchenspiel beherrschen (NIM Spiel), Vektor Darstellung beherrschen, aus Vektordarstellung Graphwerte herleiten können (XOR)
		}
			\li Vertiefungen kommen an Prüfung nicht
	\li Eindimensionale Optimierungen
		\uli{
			\li Kerzenaufgabe
			\li Aufgabe mit dem über das Feld gehen, um zur Freundin zu kommen
			\li Langange Multiplikatoren: wissen, wofür sie da sind und was man damit machen kann (Explizite Lagrange Aufgabe kommt nicht)
			\li Wissen, weshalb man Gradientenabstieg überhaupt macht (Funktion nicht gegeben, und man weis nicht, wo sich das Minima befindet)
			\li Zick-Zack Bild (Topographie (3.10) der zu optimierenden Funktion) kennen
		}
	\li Wissen, was nicht triviale Lösungsmengen sind
	\li Komprimierungsverfahren: Wissen dass Selbstähnlichkeit von Sachen ausgenutzt werden kann für Komprimierung, mehr nicht
	\li Neurnale Netze
		\uli{
			\li Biologische Grundlage von Neuronen nicht kennen
			\li Biophysik von Hodkin maxli nicht kennen
		}
	\li Perzeptron
		\uli{
			\li lineare Summierer
			\li Sinn er Feuerschwelle kennen
			\li AND-Neuron kennen
				\uli{
					\li Programm des AND Neurons lesen können
					\li Input und Outputwerte lesen können und daraus AND ablesen können
					\li Input der Feuerschwelle kennen und zuordnen können
					\li Wissen, das es auch ein Gradientenabstieg ist
					\li Sinn des Unit Steps kennnen
				}
			\li Kosinunserkennung kennen
				\uli{
					\li Sinn der Ausgabefunktion kennen
				}
			\li Wissen, dass damit XOR nicht umgesetzt werden kann
			\li Lineare Trennung kennen
			\li Werbos Perzeptron kennen und zeichnen können (mit Feuerschwellen), verstehen, dass Werbos im Raum trennt
			\li Allgemeines Netz mit zwei verborgenen Schichten kennen (out- und hiddelta kennen) (Fehlerrückverfolgungsnetz)
			\li Support-Vektormaschine: Wissen, dass sie linear trennt und Lagrange Multiplikatoren gebraucht werden. Optimale Trennlinie bestimmen können.
			\li Holographisches Neuron: wissen, dass es ein Perzeptron für komplexe Zahlen ist. \ra\ Gradientenabstieg\\
				Zeile mit Conjugate kennen. (Erkennen, dass es das holographische Neuron ist)
			\li Wissen, was einen Lernepoche ist (Einmal durch alle Übungsaufgaben durchgehen)
			\li Kohonennetzwerke: 
				\uli {
					\li Code Schnipsel mit gsi erkennen
				}
			\li Monte Carlo: 
				\uli{
					\li Wenn die Lösung besser ist (Energie kleiner ist) wird die neue Lösung genommen. Schlechtere Lösung wird mit Wahrsch. genommen, die umso kleiner ist, je schlechter die Lösung ist.
					\li Ähnliche Lernkurve wie beim Perzeptron
				}
			\li Hopfield: 
				\uli{
					\li Wissen, wie es gemacht wird 
					\li(Prototypenmuster, die mit sich selbst und den Kollegen multipliziert werden)
					\li Muster werden aufeinandern gelegt, Diagonale null gesetz
					\li Helfen neues Muster zu bearbeiten, bis es in einen Topf eines best. Prototypen fällt
					\li Mit jedem Muster wird das konträre Muster gespeichert
					\li Wissen, wie man was damit macht
					\li Wissen, wann Hopfield Netzwerke schlecht sind und das TSP Problem nicht lösen können \ra\ Musterzuordnung bringt Hopfield was
					\li Bildervergleich: Nur Fourierkoeffizienten verden verglichen.
				}
			\li Genetische Algorithmen:
				\uli{
					\li Genotyp, Phenotyp, Glücksrad kennen (Glücksrad ähnlich wie Zufall beim Monte Carlo)
					\li Code erkennen können (z.B. anhand von Parent im Code
					\li Fitnesskurve kennen (averige and maximum fitness)
					\li Genetische Programmierung kennen
				}
			\li Clustering
		}
		\important{1A4 Blatt Zusammenfassung}
		
		Fragen:
		\uli{
			\li Strategiebaum -> Ränge einzeichnen
			\li Unterschied überwachtes Lernen / unüberwachstes (Prototypen) / und Clustering / vorwärtsgerichtetes Netzwerk / nicht vorwärtsgerichtetes Netzwerk
			\li Kann ein Perzeptron ohne Bias mit der Ausgabefunktion (f(x)=1 für x>0, f(x)=0 für x <0) die Konjunktion A und B mit logischen Werten 0/1 implementieren?
			\li Unterscheidung von 100 Bildertypen mit Auflösung 1024*1024, welchen Algorithmus benützen Sie? -> Hopfield sicher nicht verwenden, Holographisches Neuron wäre gut
			\li Was, wenn die Lernrate zu gross gewählt wird.
			\li Wie wird für ein neuronales Netr die optimale Neuronenzahl bestimmt? für Hopfield, für Kohonen, für Backpropagation -> Hf 7 mal so viele wie man Pixel unterscheiden woll, für Kh 2mal
			\li Wahrheitstabelle gegeben, Architektur für Perzeptron bestimmen (Normal oder Werbos) Satz von geeigneten Gewichten bestimmen
			\li Nochmals Wahrheitstabelle, einfachstes Perzeptron oder Netzwerk bestimmen (Feuerschwellen bestimmen, Gewichte suchen)
			\li einfaches Neuronales Netz finden, das Punkte des Gebietes A von Punkte des Gebietes B trennt. -> Eine versteckte Schicht, zwei Input für x und y
			\li NIM Spiel mit 5 Haufen, geg Haufenbesetzungszahlen. Herausfinden, was zu tun ist.
			\li Welche Aufgaben von Mustern kann das Hopfield besser als das Prototypen speichern? Muster gegeben -> Ausrechnen, das Skalarprodukte 0 sind
			\li Code Snippets zuordnen
			\li Nicht stochastisches Netzwerk bestimmen
		}
		
}









\end{document}
